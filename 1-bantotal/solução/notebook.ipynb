{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio 1 - Maratona Behind the Code 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "from IPython.display import HTML, display\n",
    "from sklearn import set_config\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              ExtraTreesClassifier, RandomForestClassifier,\n",
    "                              StackingClassifier)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV  # , RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed: int = 0\n",
    "data_path: Path = Path(\"../desafio/assets/data/\")\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Junção dos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts = pd.read_csv(data_path / \"ACCOUNTS.csv\", index_col=\"ID\")\n",
    "account_cols = list(accounts.columns)\n",
    "print(account_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = pd.read_csv(data_path / \"DEMOGRAPHICS.csv\", index_col=\"ID\")\n",
    "demographic_cols = list(demographics.columns)\n",
    "print(demographic_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.read_csv(data_path / \"LOANS.csv\", index_col=\"ID\")\n",
    "loan_cols = list(loans.columns)\n",
    "print(loan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pd.read_csv(data_path / \"ANSWERS.csv\")\n",
    "df_all = pd.concat([accounts, demographics, loans], axis=1).reset_index()[answers.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Informações gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A maioria das variáveis (colunas) são numéricas, mas há algumas categóricas.\n",
    "- Com exceção da variável que identifica cada cliente (`ID`) e da variável de destino (`ALLOW`), todas têm dados faltantes (nulos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Divisão dos dados entre treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de inspecionar os dados, serão reservados alguns exemplos para teste, que não serão vistos durante a análise ou modelagem dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 500\n",
    "target = \"ALLOW\"\n",
    "df_train, df_test = train_test_split(df_all, test_size=test_size, random_state=seed, stratify=df_all[[target]])\n",
    "print(f'Dimensões dos dados de treino: {df_train.shape}')\n",
    "print(f'Dimensões dos dados de teste: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Variável destino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável destino para este desafio é a `ALLOW`, significando se um empréstimo deverá ser permitido ou não, baseado nas informações dadas. Vamos dar uma olhada em como está a distribuição dessa variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_plot = sns.countplot(\n",
    "    data=df_train, x=target, order=df_train[target].value_counts().index\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(df_train.sample(10, random_state=seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Variáveis categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_cols = list(df_train.select_dtypes(exclude='number').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(df_train.describe(include=\"O\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quantidade de valores únicos para as variáveis `CHECKING_BALANCE` e `EXISTING_SAVINGS` é relativamente grande (>30%), e observando-se os dados de exemplo, há indícios de que boa parte dos valores possam ser numéricos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_unique_treshold = 0.3\n",
    "cols_with_many_unique_values = df_train[non_numeric_cols].nunique() > max_unique_treshold * len(df_train)\n",
    "possibly_numeric_cols = list(df_train[non_numeric_cols].columns[cols_with_many_unique_values])\n",
    "possibly_numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in possibly_numeric_cols:\n",
    "    display(\n",
    "        df_train[col]\n",
    "        .value_counts()\n",
    "        .reset_index()\n",
    "        .sort_values([col, \"index\"], ascending=False)\n",
    "        .rename(columns={col: \"Quantidade\", \"index\": \"Valor\"})\n",
    "        .set_index(\"Valor\")\n",
    "        .head(3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na hora da modelagem, teremos algumas opções do que fazer com estas colunas:\n",
    "- Tratar essas colunas como variáveis numéricas, deixando que valores textuais como `NO_CHECKING` e `UNKNOWN` sejam substituídos por `NaN` (e posteriormente imputar um valor, por exemplo, zero).\n",
    "- Se for relevante distinguir o caso anterior dos zeros que já estavam nestas colunas, poderia ser criada uma nova variável binária cujo valor fosse `True` quando a variável original fosse não numérica, e `False` nos demais casos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de texto em variáveis numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por hora, apenas para visualizar estatísticas sobre os valores numéricos, valores textuais nessas colunas serão trocados por `NaN` usando um *transformer* personalizado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Instalação de um pacote extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf custom_sklearn_transformers\n",
    "!git clone git@github.com:he7d3r/custom_sklearn_transformers.git\n",
    "!git --git-dir custom_sklearn_transformers/.git archive -o custom_sklearn_transformers.zip HEAD\n",
    "!pip install --quiet --upgrade pip custom_sklearn_transformers.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso do pacote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_sklearn_transformers.transformers import ToNumeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_numeric = ToNumeric(possibly_numeric_cols, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Para criar um modelo capaz de fazer transformações nos dados de entrada, vamos criar uma `Pipeline` do `scikit-learn` e aplicar nossas transformações de pré-processamento dentro dos estágios dela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([\n",
    "    ('to_numeric', to_numeric),\n",
    "])\n",
    "clean_df = preprocessor.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estes são os valores que as outras variáveis não numéricas assumem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cat_cols = list(sorted(set(non_numeric_cols) - set(possibly_numeric_cols)))\n",
    "for col in sorted_cat_cols:\n",
    "    display(clean_df[col].value_counts().to_frame().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Todas as variáveis poderiam ser convertidas em valores numéricos usando \"One-Hot-Encoding\".\n",
    "- Alternativamente, algumas variáveis poderiam poderia ser convertidas para valores crescentes em uma ordem que aparenta ir da pior para a melhor situação (mas esta é uma avaliação subjetiva):\n",
    "  - **`CREDIT_HISTORY`**: `PRIOR_PAYMENTS_DELAYED`, `NO_CREDITS`, `CREDITS_PAID_TO_DATE`, `ALL_CREDITS_PAID_BACK`, `OUTSTANDING_CREDIT`\n",
    "  - **`OTHERS_ON_LOAN`**: `NONE`, `CO-APPLICANT`, `GUARANTOR`\n",
    "  - **`HOUSING`**: `FREE`, `RENT`, `OWN`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Variáveis numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(clean_df.describe(exclude=\"O\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinta-se livre para ver a distribuição de outras colunas do conjunto de dados, utilizar os outros conjuntos de dados, explorar as correlações entre variáveis e outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = list(clean_df.select_dtypes(include='number').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_columns', 6):\n",
    "    for col in numeric_cols:\n",
    "        display(clean_df[col].value_counts().to_frame().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Correlações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variáveis sobre as contas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_account_cols = list(sorted(set(account_cols + [target])))\n",
    "sns.pairplot(clean_df[sorted_account_cols], hue=target, palette='Set1', corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variáveis sobre dados demográficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_demographic_cols = list(sorted(set(demographic_cols + [target])))\n",
    "sns.pairplot(clean_df[sorted_demographic_cols], hue=target, palette='Set1', corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Talvez seja melhor considerar que o tipo de trabalho (`JOB_TYPE`) é uma variável categórica e fazer One-Hot-Encoding, assumindo que não exista necessariamente uma ordenação natural dos tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Variáveis sobre os empéstimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_loan_cols = list(sorted(set(loan_cols + [target])))\n",
    "sns.pairplot(clean_df[sorted_loan_cols], hue=target, palette='Set1', corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Variáveis categóricas (gráficos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in sorted_cat_cols:\n",
    "    risk_plot = sns.countplot(\n",
    "        data=df_train, y=col, order=df_train[col].value_counts().index, orient='h', hue='ALLOW', palette='Set1'\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=len(sorted_cat_cols), sharex=True, figsize=(10,20))\n",
    "for i, col in enumerate(sorted_cat_cols):\n",
    "    group_means = df_train.groupby([col])['ALLOW'].mean().rename('ALLOW_RATE').sort_values(ascending=False).to_frame()\n",
    "    sns.barplot(x='ALLOW_RATE', y=group_means.index, data=group_means, ax=axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que exploramos os dados, entendemos a importância de cada coluna e podemos fazer alterações nelas para para obter um melhor resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dados de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "O desafio espera um modelo que aceite todas as variáveis dos conjuntos de dados disponíveis (exceto a variável destino, `ALLOW`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_columns = [\n",
    "    \"ID\",\n",
    "    \"CHECKING_BALANCE\",\n",
    "    \"PAYMENT_TERM\",\n",
    "    \"CREDIT_HISTORY\",\n",
    "    \"LOAN_PURPOSE\",\n",
    "    \"LOAN_AMOUNT\",\n",
    "    \"EXISTING_SAVINGS\",\n",
    "    \"EMPLOYMENT_DURATION\",\n",
    "    \"INSTALLMENT_PERCENT\",\n",
    "    \"SEX\",\n",
    "    \"OTHERS_ON_LOAN\",\n",
    "    \"CURRENT_RESIDENCE_DURATION\",\n",
    "    \"PROPERTY\",\n",
    "    \"AGE\",\n",
    "    \"INSTALLMENT_PLANS\",\n",
    "    \"HOUSING\",\n",
    "    \"EXISTING_CREDITS_COUNT\",\n",
    "    \"JOB_TYPE\",\n",
    "    \"DEPENDENTS\",\n",
    "    \"TELEPHONE\",\n",
    "    \"FOREIGN_WORKER\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variáveis categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando a execução do método `.info()` acima, podemos ver que existem colunas do tipo `object`. O modelo do `scikit-learn` que vamos usar não é capaz de processar uma variável desse tipo. Portanto, para dar seguimento ao experimento, será utilizada a técnica de _one-hot encoding_ para tratamento de variáveis categóricas. Além disso, a coluna `ID` será desconsiderada, pois sabemos que ela não é uma informação útil para a predição (é apenas um número identificando um cliente).\n",
    "\n",
    "Primeiramente, especificaremos quais variáveis serão tratadas como categóricas e quais serão tratadas como numéricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"CREDIT_HISTORY\",\n",
    "    \"LOAN_PURPOSE\",\n",
    "    \"SEX\",\n",
    "    \"OTHERS_ON_LOAN\",\n",
    "    \"PROPERTY\",\n",
    "    \"INSTALLMENT_PLANS\",\n",
    "    \"HOUSING\",\n",
    "    \"TELEPHONE\",  # Boolean  # TODO: Impute some value instead OneHotEncoding the missing values\n",
    "    \"FOREIGN_WORKER\",  # Boolean  # TODO: Impute some value instead OneHotEncoding the missing values\n",
    "]\n",
    "numeric_features = [\n",
    "    \"CHECKING_BALANCE\",\n",
    "    \"PAYMENT_TERM\",\n",
    "    \"LOAN_AMOUNT\",\n",
    "    \"EXISTING_SAVINGS\",\n",
    "    \"EMPLOYMENT_DURATION\",\n",
    "    \"INSTALLMENT_PERCENT\",\n",
    "    \"CURRENT_RESIDENCE_DURATION\",\n",
    "    \"AGE\",\n",
    "    \"EXISTING_CREDITS_COUNT\",\n",
    "    \"JOB_TYPE\", # NOTE: This could also be considered categorical\n",
    "    \"DEPENDENTS\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualquer outra variável será desconsiderada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = numeric_features + categorical_features\n",
    "unwanted_columns = list(\n",
    "    sorted((set(challenge_columns) - set([target])) - set(features))\n",
    ")\n",
    "print(unwanted_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline de pré-processamento será atualizada para que realize transformações específicas para cada tipo de variável:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('to_numeric', to_numeric),\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "# display(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta nova versão:\n",
    "- As variáveis `CHECKING_BALANCE` e `EXISTING_SAVINGS` continuarão sendo convertidas para tipos numéricos\n",
    "- One-Hot-Encoding será aplicado às variáveis categóricas, resultando em uma coluna para cada valor\n",
    "\n",
    "A título de exemplo, a antiga coluna `SEX` foi transformada em novas colunas, uma para cada valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(df_train)\n",
    "# onehot_features = list(preprocessor.named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out())\n",
    "onehot_features = list(preprocessor.named_transformers_[\"cat\"][\"onehot\"].get_feature_names())  # scikit-learn=0.23\n",
    "clean_df = pd.DataFrame(\n",
    "    preprocessor.transform(df_train),\n",
    "    columns=numeric_features + onehot_features,\n",
    "    index=df_train.index\n",
    ")\n",
    "# clean_df.head(4)\n",
    "clean_df[clean_df.columns[clean_df.columns.str.endswith('_F') | clean_df.columns.str.endswith('_M')]].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é a lista completa das colunas criadas pelo One-Hot-Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(onehot_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variáveis faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com as etapas de pré-processamento definidas até aqui, algumas colunas ainda têm valores faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[clean_df.columns[clean_df.isnull().sum() > 0]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nestes casos, faremos apenas um tratamento simples, de imputar o valor zero nas linhas que tiverem faltando algum valor. Não necessariamente essa técnica é a melhor para se utilizar no desafio, é apenas um exemplo de como tratar o dataset.\n",
    "\n",
    "Tratamentos mais avançados, como modificação de colunas ou criação de novas colunas, serão incluídos na `Pipeline` posteriormente, se necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('to_numeric', to_numeric),\n",
    "    ('imputer', imputer),\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "# display(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(df_train)\n",
    "clean_df = pd.DataFrame(\n",
    "    preprocessor.transform(df_train),\n",
    "    columns=numeric_features + onehot_features,\n",
    "    index=df_train.index\n",
    ")\n",
    "# display(clean_df.head(2))\n",
    "# clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nova pipeline de pré-processamento zera o número de colunas com dados faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalamento de variáveis contínuas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar problemas com modelos sensíveis a diferentes escalas numéricas, vamos normalizar as variáveis numéricas para que tenham média zero e desvio padrão unitário. A título de exemplo, antes de normalizar os dados, estas são as variáveis com maior e menor desvio padrão, respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_std = clean_df.std().sort_values(ascending=False).rename('Desvio padrão').to_frame()\n",
    "display(pd.concat([top_std.head(1), top_std.tail(1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('to_numeric', to_numeric),\n",
    "    ('imputer', imputer),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "# display(preprocessor)\n",
    "\n",
    "preprocessor.fit(df_train)\n",
    "clean_df = pd.DataFrame(\n",
    "    preprocessor.transform(df_train),\n",
    "    columns=numeric_features + onehot_features,\n",
    "    index=df_train.index\n",
    ")\n",
    "# display(clean_df.head(2))\n",
    "# clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, o desvio padrão está próximo de um:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_std_normalized = clean_df.std()[top_std.index].rename('Desvio padrão').to_frame()\n",
    "display(pd.concat([top_std_normalized.head(1), top_std_normalized.tail(1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do modelo\n",
    "\n",
    "Com os dados prontos, podemos selecionar um modelo de Machine Learning para treinar com nossos dados. Nesse exemplo, vamos utilizar um modelo de classificação básico, o de Árvore de Decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier(random_state=seed)\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto! Essa pipeline agora está pronta para receber todas as variáveis do desafio, transformá-las e passar para o modelo aquelas que forem relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, separamos os dados que queremos predizer dos dados que utilizamos como informações para a predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[challenge_columns]\n",
    "y_train = df_train[target]\n",
    "X_test = df_test[challenge_columns]\n",
    "y_test = df_test[target]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "    'clf':[LinearSVC(random_state=seed)],\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    },\n",
    "    {\n",
    "    'clf': [GaussianNB()],\n",
    "    },\n",
    "    {\n",
    "    'clf': [DecisionTreeClassifier(random_state=seed)],\n",
    "    'clf__max_depth': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'clf__max_features': [0.5, 0.7, 0.9],\n",
    "    'clf__min_samples_split': [5, 10, 15],\n",
    "    'clf__min_samples_leaf': [2, 4, 8],\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    },\n",
    "    {\n",
    "    'clf': [KNeighborsClassifier()],\n",
    "    'clf__n_neighbors': [5, 10, 20, 40, 80],\n",
    "    'clf__weights': ['uniform', 'distance'],\n",
    "    },\n",
    "    {\n",
    "    'clf': [LogisticRegression(solver='liblinear', random_state=seed)],\n",
    "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    },\n",
    "    {\n",
    "    'clf': [ExtraTreesClassifier(n_estimators=10, random_state=seed)],\n",
    "    'clf__n_estimators': [50, 75, 100, 150, 200],\n",
    "    },\n",
    "    {\n",
    "    'clf': [AdaBoostClassifier(n_estimators=10, random_state=seed)],\n",
    "    'clf__n_estimators': [10, 50, 100],\n",
    "    'clf__learning_rate': [0.5, 1.0],\n",
    "    },\n",
    "    {\n",
    "    'clf':[SVC(random_state=seed)],\n",
    "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'clf__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "    },\n",
    "    {\n",
    "    'clf': [RandomForestClassifier(n_estimators=10, random_state=seed)],\n",
    "    'clf__max_depth': [5, 10],\n",
    "    'clf__n_estimators': [50, 100],\n",
    "    'clf__max_features': [0.4, 0.8],\n",
    "    'clf__min_samples_split': [2, 10],\n",
    "    'clf__min_samples_leaf': [1, 10],\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "    },\n",
    "    {\n",
    "    'clf': [BaggingClassifier(n_estimators=5, random_state=seed)],\n",
    "    'clf__base_estimator': [SVC(C=0.3, gamma=0.2, random_state=seed)],\n",
    "    'clf__n_estimators': [5, 10],\n",
    "    'clf__max_samples': [0.7, 0.9],\n",
    "    'clf__max_features': [0.4, 0.8],\n",
    "    },\n",
    "    {\n",
    "    'clf': [MLPClassifier(max_iter=10000, random_state=seed)],\n",
    "    'clf__alpha': [0.0001, 0.001, 0.01],\n",
    "    'clf__hidden_layer_sizes': [50, 75, 100],\n",
    "    'clf__learning_rate_init': [0.0001, 0.001],\n",
    "    },\n",
    "    # {\n",
    "    # 'clf': [StackingClassifier(estimators=[\n",
    "    #     ('lsvc', LinearSVC(C=0.007, penalty='l2', random_state=seed)),\n",
    "    #     ('gnb', GaussianNB()),\n",
    "    #     ('dtc', DecisionTreeClassifier(max_depth=7, max_features=0.8, min_samples_split=11, min_samples_leaf=2, criterion='gini', random_state=seed)),\n",
    "    #     ('knn', KNeighborsClassifier(n_neighbors=65, weights='uniform')),\n",
    "    #     ('lr', LogisticRegression(C=5, penalty='l2', solver='liblinear', random_state=seed)),\n",
    "    #     ('etc', ExtraTreesClassifier(n_estimators=93, random_state=seed)),\n",
    "    #     ('abc', AdaBoostClassifier(n_estimators=81, learning_rate=0.4, random_state=seed)),\n",
    "    #     ('svc', SVC(C=0.3, gamma=0.2, random_state=seed)),\n",
    "    #     ('rfc', RandomForestClassifier(n_estimators=99, max_depth=30, max_features=0.35, min_samples_split=32, min_samples_leaf=4, criterion='gini', random_state=seed)),\n",
    "    #     ('mlpc', MLPClassifier(max_iter=10000, alpha=0.25, hidden_layer_sizes=225, learning_rate_init=0.0002, random_state=seed)),\n",
    "    # ])],\n",
    "    # 'clf__final_estimator': [SVC()]\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_num = 2\n",
    "verbose = 1\n",
    "scoring = ['f1', 'accuracy', 'precision', 'recall']\n",
    "\n",
    "cv_train_size = int(((cv_num - 1) / cv_num) * len(X_train))\n",
    "cv_test_size = len(X_train) - cv_train_size\n",
    "\n",
    "\n",
    "# cv = RandomizedSearchCV(pipe, param_distributions=param_grid, n_jobs=-1, verbose=verbose, random_state=seed,\n",
    "#                           cv=cv_num, n_iter=15, scoring=scoring)\n",
    "cv = GridSearchCV(pipe, param_grid, cv=cv_num, n_jobs=-1, verbose=verbose, scoring=scoring, refit=scoring[0])\n",
    "\n",
    "start = timer()\n",
    "cv.fit(X_train, y_train)\n",
    "end = timer()\n",
    "print(f'Tempo gasto para treinar o modelo {cv_num} vezes com cada combinação de parâmetros para encontrar a melhor: {timedelta(seconds=end-start)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<h4>Dados brutos da validação cruzada</h4>'))\n",
    "print(f'Cada combinação de parâmetros foi utilizada em {cv_num} rodadas de treino com {cv_train_size} exemplos e teste com {cv_test_size}.')\n",
    "\n",
    "rank_col = 'rank_test_' + scoring[0]\n",
    "cv_df = pd.DataFrame(cv.cv_results_).sort_values(rank_col).set_index(rank_col)\n",
    "\n",
    "cols = ['mean_test_f1', 'mean_test_accuracy', 'mean_test_precision', 'mean_test_recall', 'mean_fit_time', 'params']\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(cv_df[cols].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_pickle('cv_df_10x12_12min.zip')\n",
    "print(temp.sort_values('mean_fit_time')[['mean_fit_time', 'param_clf']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.best_estimator_\n",
    "\n",
    "display(HTML(f'<h4>Maior média de {scoring[0]}</h4>'))\n",
    "display(HTML(f'''<ul>\n",
    "<li>Maior média de {scoring[0]}: {cv.best_score_:.5f} (desvio padrão: {cv.cv_results_[\"std_test_\" + scoring[0]][cv.best_index_]:.5f})</li>\n",
    "<li>Parâmetros que maximizaram a média de {scoring[0]}:<br/><code>{cv.best_params_}</code></li>\n",
    "<li>Modelo com a maior média de {scoring[0]}:</li>\n",
    "</ul>'''))\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "score = f1_score(y_test, y_pred)\n",
    "print(\n",
    "    f\"F1-score do modelo no conjunto de testes: {score:.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmd = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, labels=[0, 1], display_labels=['Negar', 'Aprovar'])\n",
    "cmd = plot_confusion_matrix(model, X_test, y_test, display_labels=['Negar', 'Aprovar'])  # scikit-learn=0.23\n",
    "cmd.ax_.set(xlabel='Previsão', ylabel='Realidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Watson Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As etapas da publicação serão colocadas em funções individuais.\n",
    "\n",
    "Primeiro, será usando um cliente da API do Watson Machine Learning para definir o espaço de publicação padrão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client(API_KEY):\n",
    "    location = 'us-south'\n",
    "    wml_credentials = {\n",
    "        \"apikey\": API_KEY,\n",
    "        \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n",
    "    }\n",
    "    return APIClient(wml_credentials)\n",
    "\n",
    "def set_default_space(client):\n",
    "    # The DEPLOYMENT_SPACE_GUID was copied from the output of\n",
    "    # client.spaces.list(limit=10)\n",
    "    DEPLOYMENT_SPACE_GUID = os.getenv(\"DEPLOYMENT_SPACE_GUID\")\n",
    "    client.set.default_space(DEPLOYMENT_SPACE_GUID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O transformador customizado será enviado para o WML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_user_package_extension(client):\n",
    "    meta_prop_pkg_extn = {\n",
    "        client.package_extensions.ConfigurationMetaNames.NAME: \"Custom_Sklearn_Transformers\",\n",
    "        client.package_extensions.ConfigurationMetaNames.DESCRIPTION: \"Extensão para transformações personalizadas\",\n",
    "        client.package_extensions.ConfigurationMetaNames.TYPE: \"pip_zip\"\n",
    "    }\n",
    "\n",
    "    # Subir o pacote\n",
    "    pkg_extn_details = client.package_extensions.store(meta_props=meta_prop_pkg_extn,\n",
    "                                                       file_path=\"custom_sklearn_transformers.zip\")\n",
    "\n",
    "    # Salvar as informações sobre o pacote\n",
    "    pkg_extn_uid = client.package_extensions.get_uid(pkg_extn_details)\n",
    "    return pkg_extn_uid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora criar uma especificação de software com o nosso pacote customizado, para que o WML possa utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_software_specification(client, pkg_extn_uid):\n",
    "    base_sw_spec_uid = client.software_specifications.get_uid_by_name(\"default_py3.8\")\n",
    "    \n",
    "    # Metadados da nova especificação de software\n",
    "    meta_prop_sw_spec = {\n",
    "        client.software_specifications.ConfigurationMetaNames.NAME: \"sw_spec_custom_sklearn_transformers\",\n",
    "        client.software_specifications.ConfigurationMetaNames.DESCRIPTION: \"Especificação de software com transformações personalizadas\",\n",
    "        client.software_specifications.ConfigurationMetaNames.BASE_SOFTWARE_SPECIFICATION: {\"guid\": base_sw_spec_uid}\n",
    "    }\n",
    "\n",
    "    # Criando a nova especificação de software e obtendo seu ID\n",
    "    sw_spec_details = client.software_specifications.store(meta_props=meta_prop_sw_spec)\n",
    "    sw_spec_uid = client.software_specifications.get_uid(sw_spec_details)\n",
    "\n",
    "    # Adicionando o pacote customizado à nova especificação\n",
    "    client.software_specifications.add_package_extension(sw_spec_uid, pkg_extn_uid)\n",
    "    return sw_spec_uid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos publicar a pipeline utilizando a especificação de software customizada que criamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_model(client, sw_spec_uid, model):\n",
    "    # Metadados do modelo\n",
    "    model_props = {\n",
    "        client.repository.ModelMetaNames.NAME: \"Pipeline customizada\",\n",
    "        client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23',\n",
    "        client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sw_spec_uid\n",
    "    }\n",
    "\n",
    "    # Publicando a Pipeline como um modelo\n",
    "    # This creates a new Model asset in the deployment space on Watson Machine Learning:\n",
    "    # https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas\n",
    "    published_model = client.repository.store_model(model=model, meta_props=model_props)\n",
    "    published_model_uid = client.repository.get_model_uid(published_model)\n",
    "    client.repository.get_details(published_model_uid)\n",
    "    return published_model_uid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que o modelo está salvo, vamos deixá-lo disponível online, para que possamos testá-lo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_deploy(client, published_model_uid):\n",
    "    # Metadados para publicação do modelo\n",
    "    metadata = {\n",
    "        client.deployments.ConfigurationMetaNames.NAME: \"Publicação do modelo customizado\",\n",
    "        client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "    }\n",
    "\n",
    "    # Publicar\n",
    "    created_deployment = client.deployments.create(published_model_uid, meta_props=metadata)\n",
    "\n",
    "    # There should be a new Deployment on the space:\n",
    "    # https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas\n",
    "    return created_deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptando um trecho de código fornecido na referência da API, na página de deployment do modelo, podemos testar a API implementada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_request_to_api(client, deployment_id):\n",
    "    meta_props = {\n",
    "        \"input_data\": [\n",
    "            {\n",
    "                \"fields\": [\n",
    "                    \"ID\",\n",
    "                    \"CHECKING_BALANCE\", \"PAYMENT_TERM\", \"CREDIT_HISTORY\", \"LOAN_PURPOSE\",\n",
    "                    \"LOAN_AMOUNT\", \"EXISTING_SAVINGS\", \"EMPLOYMENT_DURATION\", \"INSTALLMENT_PERCENT\",\n",
    "                    \"SEX\", \"OTHERS_ON_LOAN\", \"CURRENT_RESIDENCE_DURATION\", \"PROPERTY\",\n",
    "                    \"AGE\", \"INSTALLMENT_PLANS\", \"HOUSING\", \"EXISTING_CREDITS_COUNT\",\n",
    "                    \"JOB_TYPE\", \"DEPENDENTS\", \"TELEPHONE\", \"FOREIGN_WORKER\",\n",
    "                ],\n",
    "                \"values\": [\n",
    "                    [\n",
    "                        1234,\n",
    "                        None, 987.0, None, \"CAR_NEW\",\n",
    "                        4567.0, None, 5.0, 4.0,\n",
    "                        \"M\", \"NONE\", 3.0, \"SAVINGS_INSURANCE\",\n",
    "                        42.0, \"NONE\", \"OWN\", None,\n",
    "                        3.0, 1.0, 1.0, 1.0,\n",
    "                    ],\n",
    "                    [\n",
    "                        4321,\n",
    "                        \"NO_CHECKING\", 333.0, \"PRIOR_PAYMENTS_DELAYED\", \"BUSINESS\",\n",
    "                        3333.3, 111.1, 13.0, 3.0,\n",
    "                        \"F\", \"BANK\", 2.0, \"REAL_ESTATE\",\n",
    "                        33.0, \"NONE\", \"RENT\", 1.0,\n",
    "                        2.0, 2.0, 0.0, 0.0,\n",
    "                    ],\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response_scoring = client.deployments.score(deployment_id, meta_props)\n",
    "\n",
    "    # See documentation at https://cloud.ibm.com/apidocs/machine-learning?code=python#deployments-compute-predictions\n",
    "    print(\"Scoring response\")\n",
    "    display(response_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publicação do modelo e teste pela API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando estiver pronto para publicar o modelo, basta descomentar as linhas da célula abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv(find_dotenv())\n",
    "# API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "# client = get_client(API_KEY)\n",
    "# set_default_space(client)\n",
    "# pkg_extn_uid = store_user_package_extension(client)\n",
    "# sw_spec_uid = store_software_specification(client, pkg_extn_uid)\n",
    "# published_model_uid = store_model(client, sw_spec_uid, pipe)\n",
    "# created_deployment = create_deploy(client, published_model_uid)\n",
    "# deployment_uid = client.deployments.get_uid(created_deployment)\n",
    "# make_test_request_to_api(client, deployment_uid)\n",
    "\n",
    "# print(\"O modelo está publicado! \"\n",
    "#       \"Para submeter o desafio, basta acessar https://maratona.dev/challenge/1, \"\n",
    "#       \"e utilizar as credenciais abaixo para realizar a submissão:\")\n",
    "# print(\"Credenciais para envio (não compartilhe esses dados com ninguém!)\\n\\n\"\n",
    "#       f\"API key: {API_KEY}\\nDeployment ID: {deployment_uid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preenchimento do arquivo de respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['ALLOW'] = model.predict(answers)\n",
    "answers.to_csv(\"ANSWERS.csv\", index=False)\n",
    "\n",
    "with pd.option_context('display.max_columns', 6):\n",
    "    display(answers.sample(2, random_state=seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['ALLOW'].value_counts(normalize=True).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf código.zip\n",
    "!zip --exclude '*/.git/*' -rq código.zip custom_sklearn_transformers notebook.ipynb\n",
    "!ls código.zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
